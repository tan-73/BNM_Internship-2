{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DtdCkL_Nx4zs"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "52z-KC3-x-2k"
   },
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.astype(\"float32\") / 255.0\n",
    "test_images = test_images.astype(\"float32\") / 255.0\n",
    "\n",
    "train_images = train_images.reshape((60000, 28 * 28))\n",
    "test_images = test_images.reshape((10000, 28 * 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IBYcEYz9yGSr"
   },
   "outputs": [],
   "source": [
    "def compile_and_train(model):\n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=0.001,\n",
    "        decay_steps=1000,\n",
    "        decay_rate=0.96,\n",
    "        staircase=True,\n",
    "    )\n",
    "\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "    early_stop = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        train_images,\n",
    "        train_labels,\n",
    "        validation_split=0.2,\n",
    "        epochs=50,\n",
    "        batch_size=128,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1,\n",
    "    )\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "fYHjWyJlyNKQ"
   },
   "outputs": [],
   "source": [
    "model_1 = keras.Sequential([\n",
    "    keras.layers.Input(shape=(784,)),\n",
    "    keras.layers.Dense(512, kernel_constraint=keras.constraints.max_norm(3)),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dropout(0.3),\n",
    "\n",
    "    keras.layers.Dense(128, kernel_constraint=keras.constraints.max_norm(3)),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dropout(0.2),\n",
    "\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vd3a_pf6yQPU"
   },
   "outputs": [],
   "source": [
    "model_2 = keras.Sequential([\n",
    "    keras.layers.Input(shape=(784,)),\n",
    "    keras.layers.Dense(256, kernel_constraint=keras.constraints.max_norm(3)),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dropout(0.25),\n",
    "\n",
    "    keras.layers.Dense(128, kernel_constraint=keras.constraints.max_norm(3)),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dropout(0.25),\n",
    "\n",
    "    keras.layers.Dense(64, kernel_constraint=keras.constraints.max_norm(3)),\n",
    "    keras.layers.Activation(\"relu\"),\n",
    "    keras.layers.Dropout(0.2),\n",
    "\n",
    "    keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXBuYwbzyR6Q",
    "outputId": "f701e735-b5a3-4f56-cfec-76b3344e1dbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 20ms/step - accuracy: 0.8134 - loss: 0.6045 - val_accuracy: 0.9581 - val_loss: 0.1406\n",
      "Epoch 2/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 17ms/step - accuracy: 0.9521 - loss: 0.1551 - val_accuracy: 0.9702 - val_loss: 0.0994\n",
      "Epoch 3/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.9670 - loss: 0.1090 - val_accuracy: 0.9756 - val_loss: 0.0839\n",
      "Epoch 4/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 15ms/step - accuracy: 0.9751 - loss: 0.0822 - val_accuracy: 0.9747 - val_loss: 0.0856\n",
      "Epoch 5/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 16ms/step - accuracy: 0.9791 - loss: 0.0684 - val_accuracy: 0.9773 - val_loss: 0.0765\n",
      "Epoch 6/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - accuracy: 0.9827 - loss: 0.0553 - val_accuracy: 0.9782 - val_loss: 0.0768\n",
      "Epoch 7/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 25ms/step - accuracy: 0.9854 - loss: 0.0438 - val_accuracy: 0.9795 - val_loss: 0.0734\n",
      "Epoch 8/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 15ms/step - accuracy: 0.9865 - loss: 0.0423 - val_accuracy: 0.9795 - val_loss: 0.0740\n",
      "Epoch 9/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 17ms/step - accuracy: 0.9890 - loss: 0.0341 - val_accuracy: 0.9796 - val_loss: 0.0760\n",
      "Epoch 10/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 19ms/step - accuracy: 0.9897 - loss: 0.0303 - val_accuracy: 0.9804 - val_loss: 0.0735\n",
      "Epoch 1/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - accuracy: 0.7215 - loss: 0.8591 - val_accuracy: 0.9532 - val_loss: 0.1560\n",
      "Epoch 2/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9388 - loss: 0.2104 - val_accuracy: 0.9633 - val_loss: 0.1250\n",
      "Epoch 3/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9572 - loss: 0.1442 - val_accuracy: 0.9711 - val_loss: 0.1010\n",
      "Epoch 4/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - accuracy: 0.9656 - loss: 0.1139 - val_accuracy: 0.9723 - val_loss: 0.0933\n",
      "Epoch 5/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9721 - loss: 0.0937 - val_accuracy: 0.9741 - val_loss: 0.0939\n",
      "Epoch 6/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9770 - loss: 0.0783 - val_accuracy: 0.9738 - val_loss: 0.0885\n",
      "Epoch 7/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 10ms/step - accuracy: 0.9760 - loss: 0.0784 - val_accuracy: 0.9778 - val_loss: 0.0801\n",
      "Epoch 8/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 13ms/step - accuracy: 0.9806 - loss: 0.0662 - val_accuracy: 0.9773 - val_loss: 0.0849\n",
      "Epoch 9/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9822 - loss: 0.0562 - val_accuracy: 0.9757 - val_loss: 0.0879\n",
      "Epoch 10/50\n",
      "\u001b[1m375/375\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 11ms/step - accuracy: 0.9838 - loss: 0.0542 - val_accuracy: 0.9778 - val_loss: 0.0828\n"
     ]
    }
   ],
   "source": [
    "history_3 = compile_and_train(model_1)\n",
    "history_4 = compile_and_train(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "433Wxfn9yTdX"
   },
   "outputs": [],
   "source": [
    "def plot_history(histories, names):\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    for history, name in zip(histories, names):\n",
    "        plt.plot(history.history[\"loss\"], label=f\"{name} Train\")\n",
    "        plt.plot(history.history[\"val_loss\"], label=f\"{name} Val\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    for history, name in zip(histories, names):\n",
    "        plt.plot(history.history[\"accuracy\"], label=f\"{name} Train\")\n",
    "        plt.plot(history.history[\"val_accuracy\"], label=f\"{name} Val\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 500
    },
    "id": "8XiH4q6n3qVh",
    "outputId": "f2d2e752-907f-42b7-cc35-7c95db717907"
   },
   "source": [
    "plot_history([history_3, history_4], [\"Model 1\", \"Model 2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "y0DqPP8o3we9"
   },
   "outputs": [],
   "source": [
    "test_loss_3, test_acc_3 = model_1.evaluate(test_images, test_labels, verbose=0)\n",
    "test_loss_4, test_acc_4 = model_2.evaluate(test_images, test_labels, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gFlk9fnC3ywD",
    "outputId": "7c872509-e85f-4532-d7e9-109616dd9758"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1 - Test Accuracy: 0.9810, Test Loss: 0.0623\n",
      "Model 2 - Test Accuracy: 0.9780, Test Loss: 0.0750\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model 1 - Test Accuracy: {test_acc_3:.4f}, Test Loss: {test_loss_3:.4f}\")\n",
    "print(f\"Model 2 - Test Accuracy: {test_acc_4:.4f}, Test Loss: {test_loss_4:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Metrics | Model 1 (Droput) | Model 2 (MaxNorm) |\n",
    "|:--------:|:--------:|:--------:|\n",
    "|  **Regularization**   |  Droput (0.3 & 0.2)   |  MaxNorm constraint (limit = 3.0)   |\n",
    "|  **Final training loss**   |  ~0.04   |  ~0.035   |\n",
    "|  **Final validation loss**   |  0.05   |  0.045   |\n",
    "|  **Test accuracy**    |  97.80%   |  98.10%   |\n",
    "|  **Test loss**   |  0.0750   |  0.0623   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Model 3 – Dropout Regularization\n",
    "\n",
    "- Applied **Dropout** with 30% and 20% dropout rates in two hidden layers.\n",
    "- Dropout helps reduce overfitting by randomly deactivating neurons during training.\n",
    "- Test accuracy reached **97.80%**, which is strong, but the model exhibited **higher test loss (0.0750)**.\n",
    "- Slightly lower generalization performance and prediction confidence compared to MaxNorm.\n",
    "\n",
    "---\n",
    "\n",
    "####  Model 4 – MaxNorm Constraint Regularization\n",
    "\n",
    "- Used **MaxNorm constraint** to limit the L2 norm of weight vectors in Dense layers.\n",
    "- Achieved **higher test accuracy (98.10%)** and **lower test loss (0.0623)** than the Dropout model.\n",
    "- MaxNorm helped the model learn stable representations while effectively preventing overfitting.\n",
    "- Demonstrated **better generalization** and **confidence in predictions**.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
